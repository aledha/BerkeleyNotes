Questions 5.4 (Part 1 only), 5.5, 5.14, 5.18
Alexander Hatle
![[Pasted image 20231106134446.png|800]]

![[Pasted image 20231106135155.png|700]]
![[Pasted image 20231107155117.png|700]]
Using CF,
$$\begin{align*}
\alpha _{j}&= \min_{\mathbf S^{n-j+1}} \max_{0≠s \in \mathbf S^{n-j+1}} \frac{s^{T}As}{s^{T}s}\\
&\ge  \min_{\mathbf S^{n-j}} \max_{0≠s_{1} \in \mathbf S^{n-j}} \frac{1}{s_{1}^{T}s_{1}} \cdot \begin{bmatrix}s_{1}^{T}  & 0\end{bmatrix}\begin{bmatrix}H & b\\
b^{T} & u\end{bmatrix}\begin{bmatrix}s_{1}\\
0\end{bmatrix}\\
\end{align*}$$
where we get an inequality since have added the extra restriction that the last entry of $s$ should be zero. Then
$$\begin{align*}
\alpha _{j}&\ge \min_{\mathbf S^{n-j}} \max_{0≠s \in \mathbf S^{n-j}} \frac{s_{1}^{T}Hs_{1}}{s_{1}^{T}s_{1}}\\
\alpha _{j}&\ge \theta _{j}
\end{align*}$$
We also have that 
$$\begin{align*}
\alpha _{j}&= \max_{\mathbf R^{j}}\min_{r \in \mathbf R^{j}} \frac{r^{T}Ar}{r^{T}r}\\
	&\le \max_{\mathbf R^{j-1}}\min_{r_{1}\in \mathbf R^{j-1}} \frac{1}{r^{T}_{1}r_{1}} \begin{bmatrix}r_{1}^{T} & 0\end{bmatrix}\begin{bmatrix}H & b\\
b^{T} & u\end{bmatrix}\begin{bmatrix}r_{1}\\
0\end{bmatrix}
\end{align*}$$
where we again have restricted that $r=\begin{bmatrix}r_{1} \\ 0\end{bmatrix}$.
$$\begin{align*}
\alpha _{j}&\le \max_{\mathbf R^{j-1}}\min_{r_{1}\in \mathbf R^{j-1}} \frac{r_{1}^{T}Hr_{1}}{r_{1}^{T}r_{1}}\\
\alpha _{j}&\le \theta _{j-1}
\end{align*}$$
Combining the two inequalities, we get the interlacing theorem
$$\alpha _{n}\le \theta _{n-1}\le \dots \le \theta _{i}\le \alpha _{i}\le \theta _{i-1}\le \dots \le \theta _{1}\le \alpha _{1}$$

<div style="page-break-after: always;"></div>

![[Pasted image 20231106134543.png|800]]
$$\begin{align*}
\lambda _{j}&= \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \frac{s^{T}(A+H)s}{s^{T}s}\\
&= \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \left(\frac{s^{T}As}{s^{T}s}+\frac{s^{T}Hs}{s^{T}s}\right)\\
&\le \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \left(\frac{s^{T}As}{s^{T}s}+ \theta _{1}\right),
\end{align*}$$
since $\frac{s^{T}Hs}{s^{T}s}$ cannot be larger that the largest eigenvalue of $H$, $\theta _{1}$. Then,
$$\lambda _{j} \le \alpha _{j}+\theta _{1}.$$
The other way is similar,
$$\begin{align*}
\lambda _{j}&=  \max_{\mathbf R^{j}}\min_{0≠r \in \mathbf R^{j}}\frac{s^{T}(A+H)s}{s^{T}s}\\
&= \max_{\mathbf R^{j}}\min_{0≠r \in \mathbf R^{j}}\left(\frac{s^{T}As}{s^{T}s}+\frac{s^{T}Hs}{s^{T}s} \right)\\
&\ge \max_{\mathbf R^{j}}\min_{0≠r \in \mathbf R^{j}}\left(\frac{s^{T}As}{s^{T}s}+\theta _{n} \right),
\end{align*}$$
since $\frac{s^{T}Hs}{s^{T}s}$ cannot be smaller that the smallest eigenvalue of $H$, $\theta _{n}$. Then we have the desired result
$$\alpha _{j}+ \theta _{n}\le \lambda _{j}\le \alpha _{j}+\theta _{1}.$$

If $H$ is positive definite, it's eigenvalues are strictly positive, and it's smallest, $\theta _{n}>0$. From the inequality,
$$\begin{align*}
\alpha _{j}+\theta _{n} &\le \lambda _{j}\\
\alpha _{j}&< \lambda _{j}.
\end{align*}$$
<div style="page-break-after: always;"></div>

![[Pasted image 20231106134517.png|800]]
![[Pasted image 20231106134806.png|800]]
I assume that $x,y \in \mathbb{R}^{n}$, and that $I+xy^{T}$ is square. We see that $x$ is a right eigenvector
$$\begin{align*}
(I+xy^{T})x&=x+xy^{T}x\\
&= (1+y^{T}x)x,
\end{align*}$$
and that $y$ is a left eigenvector
$$\begin{align*}
y^{T}(I+xy^{T})&= y^{T}+y^{T}xy^{T}\\
&= (1+y^{T}x)y^{T},
\end{align*}$$
with eigenvalue $1+y^{T}x$.
The other eigenvectors can be found by
$$(I+xy^{T})z=z+xy^{T}z,$$
and setting $z$ orthogonal to $y$, such that $y^{T}z=0$, which is an $n-1$ dimensional subspace of $\mathbb{R}^{n}$. Thus the corresponding eigenvalues are 1, with multiplicity $n-1$.
The determinant of a square matrix is the product of the eigenvalues, and therefore
$$\text{ det}(I+xy^{T})=(1+y^{T}x)\cdot 1\cdots1=1+y^{T}x.$$
<div style="page-break-after: always;"></div>

![[Pasted image 20231106134502.png|800]]
The thin formulation of the SVD of $A$ is
$$A=U \Sigma V^{T},$$
where $U \in \mathbb{R}^{m \times n},\Sigma  \in \mathbb{R}^{n \times n},V \in \mathbb{R}^{n \times n}$. This is unique if one chooses to order the singular values in ascending/descending order.

Let $P=V \Sigma V^{T} \in \mathbb{R}^{n \times n}$ and $Q=U V^{T} \in \mathbb{R}^{m \times n}$, then
$$QP= UV^{T}V \Sigma V^{T}=U \Sigma V^{T}=A.$$
And the columns of $Q$ are orthonormal,
$$Q^{T}Q=VU^{T}UV^{T}=I,$$
and $P$ is positive semidefinite since
$$xPx^{T}=xV \Sigma V^{T}x^{T}= y \Sigma y^{T}\ge0$$
since $\Sigma=\text{diag}(\sigma _{1},\dots \sigma _{n})$ and the singular values are non-negative.

Assume that $A$ is nonsingular, which implies that no singular values are zero. Notice that 
$$(P)_{ij}=(V \Sigma V^{T})_{ij}=\sum_{k=1}^{n}\sigma _{k}V_{ik}V_{jk}$$
and that the SVD is unique up to the ordering of the singular values, and this reflects the ordering of the columns of $V$. A different order of the singular values will simply result in a different order of summation. Therefore, each index of $P$ is uniquely determined. 
For $Q$, we have that
$$(Q)_{ij}=(UV^{T})_{ij}=\sum_{k=1}^{n}U_{ik}V_{jk}.$$
Again, the ordering of singular values reflects the ordering of the columns of $U$ and $V$. A different order of singular values will result in a different order of summation. $Q$ is therefore also uniquely determined.