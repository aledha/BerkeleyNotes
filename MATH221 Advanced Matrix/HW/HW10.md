Questions 5.4 (Part 1 only), 5.5, 5.14, 5.18

![[Pasted image 20231106134446.png|800]]
Errata: "Page 260, Question 5.4. It should be stated that B and C are real-valued. (Nicholas Knight)"
![[Pasted image 20231106135155.png|700]]
![[Pasted image 20231107155117.png|700]]
https://www.cse.psu.edu/~b58/cse550/svd.pdf
Suppose that where $v_{j}$ are the eigenvectors of $H$. Let
$$w_{i}=\begin{bmatrix}v_{i} \\ 0\end{bmatrix} $$

$$ \begin{align*}
\alpha _{j}&= \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \frac{s^{T}As}{s^{T}s}\\
&= \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \frac{1}{s^{T}s} \cdot \begin{bmatrix}s_{1}^{T}  & s_{n}\end{bmatrix}\begin{bmatrix}H & b\\
b^{T} & u\end{bmatrix}\begin{bmatrix}s_{1}\\
s_{n}\end{bmatrix}\\
		&= \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \frac{1}{s_{1}^{T}s_{1}+s_{n}^{2}}\cdot \begin{bmatrix}s_{1}^{T} & s_{n}\end{bmatrix}\begin{bmatrix}Hs_{1}+bs_{n}\\
b^{T}s_{1}+us_{n}\end{bmatrix}\\
&= \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \frac{1}{s_{1}^{T}s_{1}+s_{n}^{2}}\cdot (s_{1}^{T}Hs_{1}+s_{1}^{T}bs_{n}+s_{n}b^{T}s_{1}+s_{n}us_{n})\\
&= \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \frac{1}{s_{1}^{T}s_{1}+s_{n}^{2}}\cdot (s_{1}^{T}Hs_{1}+2s_{n}s_{1}^{T}b+us^{2}_{n})
\end{align*}$$

![[Pasted image 20231106134543.png|800]]
$$\begin{align*}
\lambda _{j}&= \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \frac{s^{T}(A+H)s}{s^{T}s}\\
&= \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \left(\frac{s^{T}As}{s^{T}s}+\frac{s^{T}Hs}{s^{T}s}\right)\\
&\le \min_{\mathbf S^{n-j-1}} \max_{0≠s \in \mathbf S^{n-j-1}} \left(\frac{s^{T}As}{s^{T}s}+ \theta _{1}\right),
\end{align*}$$
since $\frac{s^{T}Hs}{s^{T}s}$ cannot be larger that the largest eigenvalue of $H$, $\theta _{1}$. Then,
$$\lambda _{j} \le \alpha _{j}+\theta _{1}.$$
The other way is similar,
$$\begin{align*}
\lambda _{j}&=  \max_{\mathbf R^{j}}\min_{0≠r \in \mathbf R^{j}}\frac{s^{T}(A+H)s}{s^{T}s}\\
&= \max_{\mathbf R^{j}}\min_{0≠r \in \mathbf R^{j}}\left(\frac{s^{T}As}{s^{T}s}+\frac{s^{T}Hs}{s^{T}s} \right)\\
&\ge \max_{\mathbf R^{j}}\min_{0≠r \in \mathbf R^{j}}\left(\frac{s^{T}As}{s^{T}s}+\theta _{n} \right),
\end{align*}$$
since $\frac{s^{T}Hs}{s^{T}s}$ cannot be smaller that the smallest eigenvalue of $H$, $\theta _{n}$. Then we have the desired result
$$\alpha _{j}+ \theta _{n}\le \lambda _{j}\le \alpha _{j}+\theta _{1}.$$

If $H$ is positive definite, it's eigenvalues are strictly positive, and it's smallest, $\theta _{n}>0$. From the inequality,
$$\begin{align*}
\alpha _{j}+\theta _{n} &\le \lambda _{j}\\
\alpha _{j}&< \lambda _{j}.
\end{align*}$$

![[Pasted image 20231106134517.png|800]]
![[Pasted image 20231106134806.png|800]]
I assume that $x,y \in \mathbb{R}^{n}$, and that $I+xy^{T}$ is square. We see that $x$ is a right eigenvector
$$\begin{align*}
(I+xy^{T})x&=x+xy^{T}x\\
&= (1+y^{T}x)x,
\end{align*}$$
and that $y$ is a left eigenvector
$$\begin{align*}
y^{T}(I+xy^{T})&= y^{T}+y^{T}xy^{T}\\
&= (1+y^{T}x)y^{T},
\end{align*}$$
with eigenvalue $1+y^{T}x$.
The other eigenvectors can be found by
$$(I+xy^{T})z=z+xy^{T}z,$$
and setting $z$ orthogonal to $y$, such that $y^{T}z=0$, which is an $n-1$ dimensional subspace of $\mathbb{R}^{n}$. Thus the corresponding eigenvalues are 1, with multiplicity $n-1$.
The determinant of a square matrix is the product of the eigenvalues, and therefore
$$\text{ det}(I+xy^{T})=(1+y^{T}x)\cdot 1\cdots1=1+y^{T}x.$$



![[Pasted image 20231106134502.png|800]]
The thin formulation of the SVD of $A$ is
$$A=U \Sigma V^{T},$$
where $U \in \mathbb{R}^{m \times n},\Sigma  \in \mathbb{R}^{n \times n},V \in \mathbb{R}^{n \times n}$. This is unique if one chooses to order the singular values in ascending/descending order.

Let $P=V \Sigma V^{T} \in \mathbb{R}^{n \times n}$ and $Q=U V^{T} \in \mathbb{R}^{m \times n}$, then
$$QP= UV^{T}V \Sigma V^{T}=U \Sigma V^{T}=A.$$
And the columns of $Q$ are orthonormal,
$$Q^{T}Q=VU^{T}UV^{T}=I,$$
and $P$ is positive semidefinite since
$$xPx^{T}=xV \Sigma V^{T}x^{T}= y \Sigma y^{T}\ge0$$
since $\Sigma=\text{diag}(\sigma _{1},\dots \sigma _{n})$ and the singular values are non-negative.

If $A$ is non-singular (square and invertible), then the singular values are strictly positive. I don't know how to use this to show that the decomposition is unique.
I know we just need to prove that A=Q1P1=Q2P2A=Q1​P1​=Q2​P2​, then Q1=Q2Q1​=Q2​ or P1=P2P1​=P2​. Edit: never mind, I just didn't realize positive definite implies symmetric